\documentclass[conf]{new-aiaa}
%\documentclass[journal]{new-aiaa} for journal papers
\usepackage[utf8]{inputenc}
\usepackage{parskip}
\usepackage{listings}
\usepackage{xcolor}

% Define a style for code
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}
\lstset{style=mystyle}

\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{longtable,tabularx}
\usepackage[capitalise]{cleveref}
\usepackage{nomencl}
\usepackage{longtable}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\usepackage{matlab-prettifier}
\usepackage{booktabs}
\usepackage[subfigure]{tocloft} 
\usepackage{subfig}
\usepackage[labelfont=bf, textfont=bf, font=small]{caption}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{makecell}
\usepackage{tocloft}
\usepackage{subfig}
\usepackage[section]{placeins}
% Used for inserting MATLAB code into appendix
\usepackage{listings}
\usepackage{color}


\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
  language=Matlab,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  frame=L,
  basicstyle={\small\ttfamily},
  numbers=left, 
  numbersep=10pt,  
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\begin{document}
\setlength{\parindent}{2em}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.48mm}}

\center
 
\textsc{\LARGE University of Colorado - Boulder}\\[1cm]
\textsc{\Large Statistical Estimation of Dynamical Systems: ASEN5044}\\[0.5cm] % Major heading such as course name
\textsc{\large 11/28/2025}\\[0.5cm] % Minor heading such as course title

\HRule \\[0.4cm]
{ \huge \bfseries Cooperative Air-Ground Robot Localization Progress Report 1}\\[0.4cm] 
\HRule \\[0.75cm]

\begin{minipage}{0.26\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Aaron Pineda \textsc{aaron.pineda@colorado.edu}
\end{flushleft}
\begin{flushleft} \large
\emph{Author:}\\
Jeremy Aubert \textsc{jeremy.aubert@colorado.edu}
\end{flushleft}
\begin{flushleft} \large
\emph{Author:}\\
Landry Matthews \textsc{landry.matthews@colorado.edu}
\end{flushleft}


\end{minipage}
\begin{minipage}{0.3\textwidth}
\begin{flushright} \large
\emph{Professor:} \\
Professor \textsc{Khosro GhobadiFar} 
\end{flushright}
\end{minipage}\\[2cm]

\includegraphics[width=0.65\textwidth]{watermark.png}\\
 
\vfill

\begin{abstract}

\end{abstract}
\end{titlepage}

\section{Introduction}
\vspace{1em} % adds extra space
 Accurate and resilient localization remains a fundamental challenge for autonomous robotic systems operating in outdoor environments. While Global Positioning System (GPS) technology has become the standard solution for navigation, its reliability can be compromised in urban canyons, forested regions, mountainous terrain, or under intentional interference such as jamming and spoofing. These vulnerabilities highlight the need for complementary localization strategies that can sustain performance when GPS signals are degraded or unavailable.

\par One promising approach is cooperative localization, in which multiple robots share relative measurements to improve their individual and collective state estimates. By leveraging peer-to-peer sensing and communication, robots can augment their own navigation solutions with information derived from nearby teammates. This paradigm is particularly relevant for heterogeneous teams, such as ground and aerial vehicles, where each platform has distinct sensing capabilities and environmental constraints. Cooperative localization not only enhances robustness but also enables multi-vehicle teams to operate effectively in GPS-denied environments.

\par
 This project investigates cooperative localization in a simplified two-dimensional scenario involving an unmanned ground vehicle (UGV) and an unmanned aerial vehicle (UAV). The UGV, modeled as a Dubins car with steering and velocity inputs, traverses the ground while tracking the UAV. The UAV, modeled as a Dubins unicycle with velocity and turn-rate inputs, flies a continuous trajectory overhead. During their encounter, the UAV maintains reliable GPS measurements, while the UGV relies solely on relative measurements to the UAV. A centralized estimator is assumed to fuse these measurements, providing a benchmark for evaluating decentralized algorithms. 

\par
 The focus of this work is the design, implementation, and validation of nonlinear stochastic filters; specifically a linearized Kalman filter (LKF) and an extended Kalman filter (EKF) to estimate the joint state of the UAV–UGV system. Monte Carlo truth-model testing, along with normalized estimation error squared (NEES) and normalized innovation squared (NIS) chi-square consistency tests, are employed to tune and assess filter performance. By analyzing estimation accuracy, statistical consistency, and robustness to noise, this project aims to demonstrate the feasibility of cooperative localization as a means of overcoming GPS limitations in multi-robot systems.

\section{Deterministic System Analysis}
\vspace{1em} % adds extra space
\subsection{Defining the Physical Dynamical System}
\vspace{1em} % adds extra space
The cooperative localization scenario studied in this project involves a heterogeneous, non‑holonomic team composed of a ground robot and an aerial robot operating in a two‑dimensional plane. The ground platform is a four‑wheeled unmanned ground vehicle (UGV) modeled using Dubins‑car kinematics. Its motion is governed by steering and velocity inputs, reflecting the constraints of a car‑like vehicle. The aerial platform is a fixed‑wing unmanned aerial vehicle (UAV) modeled using Dubins‑unicycle kinematics. It follows a continuous trajectory at constant altitude above the UGV, with its motion controlled by forward velocity and turn rate inputs. Both vehicles are independently actuated, but their interaction arises through relative sensing.

\par
During their encounter, the vehicles exchange relative measurements. The UGV, which cannot rely on GPS in this scenario, disables its receiver and depends entirely on relative observations of the UAV. In contrast, the UAV maintains access to reliable GPS measurements throughout the encounter. This asymmetry reflects realistic operating conditions in which ground robots may experience degraded satellite coverage, while aerial robots retain line‑of‑sight to satellites.

\par
Although the system is designed to support decentralized cooperative localization, where each vehicle estimates both its own state and that of its teammate, the focus of this project is on a centralized estimator. In this configuration, a base station collects measurements from both the UGV and UAV and fuses them to estimate the combined system state. This centralized solution serves as a benchmark or “gold standard” against which decentralized algorithms can be evaluated. The models employed, as shown in Figure 1, are deliberately simplified yet nonlinear, capturing the essential kinematic behavior of each vehicle while enabling rigorous testing of stochastic filtering methods.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Problem_Setup.png}
    \caption{Modeled Dynamics.}
    \label{fig:Modeled Dynamics vs Nonlinear Dynamics}
\end{figure}

\par
As previously mentioned, the kinematics of the UGV are modeled as a simple steerable Dubins style 4-wheeled car. The cars wheelbase is defined by the variable L, and has an inertial frame defined by the East position \(\xi_g\) and the North position \(\eta_g\).  The UGV's control inputs are \(u_g = [v_g,\phi_g]^T\), where \(v_g\) (m\s) is the linear velocity and \(\phi_g\) (rad\s) is the angular rate of the car. The kinematics are defined by the nonlinear equations of motion below.

\begin{align}
\dot{\xi}_g &= v_g \cos \theta_g + \tilde{w}_{x,g}, \label{eq:ugv_x}\\
\dot{\eta}_g &= v_g \sin \theta_g + \tilde{w}_{y,g}, \label{eq:ugv_y}\\
\dot{\theta}_g &= \frac{v_g}{L} \tan \phi_g + \tilde{w}_{\omega,g}. \label{eq:ugv_theta}
\end{align}

\par
The Fixed wing UAV kinematic model was styled after the Dubins Unicycle, which like the Dubins car is governed mainly by linear and angular velocity. The UAV's inertial frame is defined by the East position \(\xi_a\) and the North position \(eta_a\) and a heading of \(\theta_a\). The UAV's control inputs are given as \(u_a = [v_A,w_a]^T\), where \(v_a\)(m\s) is the system's linear velocity, and \(w_a\)(rad\s) is the systems angular velocity. The full kinematics are shown in the nonlinear equations of motion below.

\begin{align}
\dot{\xi}_a &= v_a \cos \theta_a + \tilde{w}_{x,a}, \label{eq:uav_x}\\
\dot{\eta}_a &= v_a \sin \theta_a + \tilde{w}_{y,a}, \label{eq:uav_y}\\
\dot{\theta}_a &= \omega_a + \tilde{w}_{\omega,a}. \label{eq:uav_theta}
\end{align}

\par
The combined states of the system, as well as the control and disturbance inputs are: 

\[
x(t) =
\begin{bmatrix}
\xi_g \\ \eta_g \\ \theta_g \\ \xi_a \\ \eta_a \\ \theta_a
\end{bmatrix},
\quad
u(t) =
\begin{bmatrix}
u_g \\ u_a
\end{bmatrix},
\quad
\tilde{w}(t) =
\begin{bmatrix}
\tilde{w}_g \\ \tilde{w}_a
\end{bmatrix}.
\]

where \(
\tilde{w}_g(t) = [\tilde{w}_{x,g}, \tilde{w}_{y,g}, \tilde{w}_{\omega,g}]^T\) describes the process noise of the UGV states and \(\tilde{w}_a(t) = [\tilde{w}_{x,a}, \tilde{w}_{y,a}, \tilde{w}_{\omega,a}]^T\) describes the process noise of the UAV states.

\par
The sensing model \(h(t)\) for the system is given by noisy ranges and azimuth angles of the UGV relative to the UAV, noisy azimuth angles of the UAV relative to the UGV, and noisy UAV GPS measurements, \[
y(t) =
\begin{bmatrix}
\tan^{-1}\!\left(\frac{\eta_a - \eta_g}{\xi_a - \xi_g}\right) - \theta_g \\[1ex]
\sqrt{(\xi_g - \xi_a)^2 + (\eta_g - \eta_a)^2} \\[1ex]
\tan^{-1}\!\left(\frac{\eta_g - \eta_a}{\xi_g - \xi_a}\right) - \theta_a \\[1ex]
\xi_a \\[1ex]
\eta_a
\end{bmatrix}
+ \tilde{v}(t),
\]

where \(\tilde{v}(t)\) is the sensor measurement noise that is modeled as Additive White Gaussian Noise(AWGN).

\subsection{Linearizing the CT model}
\vspace{1em} % adds extra space
With the equations of motion laid out as well as the nominal system parameters listed below; 
\begin{description}
    \item[UGV wheelbase length:] $L = 0.5 \,\text{m}$
    \item[UGV steering angle range:] $\phi_g \in \left[-\tfrac{5\pi}{12}, \tfrac{5\pi}{12}\right] \,\text{rad}$
    \item[UGV maximum speed:] $v_{g,\max} = 3 \,\text{m/s}$
    \item[UAV turn rate range:] $\omega_a \in \left[-\tfrac{\pi}{6}, \tfrac{\pi}{6}\right] \,\text{rad/s}$
    \item[UAV velocity range:] $v_a \in [10, 20] \,\text{m/s}$
    \item[Nominal UGV initial state:] $(\xi_g, \eta_g) = (10,0)$, $\theta_g = \tfrac{\pi}{2} \,\text{rad}$
    \item[Nominal UGV trajectory:] $v_g = 2 \,\text{m/s}$, $\phi_g = -\tfrac{\pi}{18} \,\text{rad}$
    \item[Nominal UAV initial state:] $(\xi_a, \eta_a) = (-60,0)$, $\theta_a = -\tfrac{\pi}{2} \,\text{rad}$
    \item[Nominal UAV trajectory:] $v_a = 12 \,\text{m/s}$, $\omega_a = \tfrac{\pi}{25} \,\text{rad/s}$
\end{description}

We were able to then derive our CT jacobians. \[
A(x,u) =
\begin{bmatrix}
0 & 0 & -v_g \sin\theta_g & 0 & 0 & 0 \\
0 & 0 & \phantom{-}v_g \cos\theta_g & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & -v_a \sin\theta_a \\
0 & 0 & 0 & 0 & 0 & \phantom{-}v_a \cos\theta_a \\
0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\],

\[
B(x,u) =
\begin{bmatrix}
\cos\theta_g & 0 & 0 & 0 \\
\sin\theta_g & 0 & 0 & 0 \\
\frac{1}{L}\tan\phi_g & \frac{v_g}{L}\sec^2\!\phi_g & 0 & 0 \\
0 & 0 & \cos\theta_a & 0 \\
0 & 0 & \sin\theta_a & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\]
\[
H(x) =
\begin{bmatrix}
\frac{\eta_a - \eta_g}{(\xi_a - \xi_g)^2 (1+\frac{\eta_a - \eta_g}{\xi_a - \xi_g})} & -\frac{1}{(\xi_a - \xi_g)(1+\frac{\eta_a - \eta_g}{\xi_a - \xi_g}} & -1 & -\frac{\eta_a - \eta_g}{(\xi_a - \xi_g)^2 (1+\frac{\eta_a - \eta_g}{\xi_a - \xi_g})} & \phantom{-}\frac{1}{(\xi_a - \xi_g) (1+\frac{\eta_a - \eta_g}{\xi_a - \xi_g})} & 0 \\[6pt]
-\frac{2(\xi_a - \xi_g)}{\sqrt{(\xi_a - \xi_g)^2 + (\eta_a - \eta_g)^2)}} & -\frac{2(\eta_a - \eta_g)}{(\xi_a - \xi_g)^2 + (\eta_a - \eta_g)^2)} & 0 &\frac{2(\xi_a - \xi_g)}{(\xi_a - \xi_g)^2 + (\eta_a - \eta_g)^2)} & \frac{2(\eta_a - \eta_g)}{(\xi_a - \xi_g)^2 + (\eta_a - \eta_g)^2)} & 0 \\[6pt]
-\frac{\eta_g - \eta_a}{\xi_g - \xi_a)^2 (1+\frac{\eta_g - \eta_a}{\xi_g - \xi_a})} & \frac{1}{\xi_g - \xi_a)(1+\frac{\eta_g - \eta_a}{\xi_g - \xi_a})} & 0 & \frac{\eta_g - \eta_a}{\xi_g - \xi_a)^2 (1+\frac{\eta_g - \eta_a}{\xi_g - \xi_a})} & \phantom{-}\frac{1}{\xi_g - \xi_a)(1+\frac{\eta_g - \eta_a}{\xi_g - \xi_a})} & -1 \\[6pt]
0 & 0 & 0 & 1 & 0 & 0 \\[3pt]
0 & 0 & 0 & 0 & 1 & 0
\end{bmatrix}
\]

 However, because the system is time varying the jacobians change with every time step. Because of this, our normal methods of determining stability and observability will not work here. Therefore we skipped that analysis in this report. Due to the system being LTV our approach was to determine the CT dynamics of the system, \(\tilde{A}\), \(\tilde{B}\) and \(\tilde{C}\), then convert to DT dynamics using the Forward Euler method to find \(\tilde{F}\) and \(\tilde{G}\). \(\tilde{H}\) is obtained from linearizing the nonlinear measurement function \(h(x)\). In the linear case \(\tilde{H}\) reduces to \(\tilde{C}\) . The equations for \(\tilde{F}\) and \(\tilde{G}\) were derived in lecture to be \(\tilde{F} = I + \Delta t\tilde{A}\) and \(\tilde{G} = \Delta t\tilde{B}\). After converting the system to DT, we then linearized around a nominal trajectory at each time step. Since the nominal trajectory was different at each timestep of the system, the nonlinear equations needed to be integrated over the desired range of 100 seconds using \(solve IVP\) from the python library scipy. For this we used the initial nominal state condition that was provided of \(x_0 = [10, 0, \tfrac{\pi}{2}, -60, 0, -\tfrac{\pi}{2}]\). For each timestep the computed nominal trajectory was used to evaluate each of the state matrices and determine the future steps perturbation through the equation \( \delta x(k+1) \approx \tilde{F}_{nom}(k)\delta x(k)\). For this part contributions from noise and control inputs were ignored for perturbation vectors.

\subsection{Simulating Linearized DT Dynamics}
\vspace{1em} % adds extra space
One thing to be noted is that the angles for the UGV and UAV had to be bounded within the specified limitations, and were bounded as such in the script to generate the correct plots.
Firstly, the systems nonlinear dynamics were plotted against the estimated linearized dynamics. The initial perturbation used was \(x_0 = [0; 1; 0; 0; 0; 0.1]\) with nominal control inputs and no process or measurement noise in the simulation. The nonlinear states were generated from Python's ODE integrator \(solve_IVP\) for 100 seconds(1000 time steps) which was overlayed with the simulated linearized dynamics. Following this plot is the simulated measurement data and true measurement data, following finally with the perturbation dynamics.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Dynamics.png}
    \caption{Modeled Dynamics.}
    \label{fig:Modeled Dynamics vs Nonlinear Dynamics}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Measurements.png}
    \caption{Measurements.}
    \label{fig:Simulated Data vs True Data}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Perturbations.png}
    \caption{Perturbation Dynamics.}
    \label{fig:Simulated Perturbations}
\end{figure}



\section{Stochastic Nonlinear Filtering: Linearized Kalman Filter}

\subsection{Implementation of a Linearized Kalman Filter}
This section describes the steps taken to implement the DT linearized kalman filter for the cooperative localization problem.  We start with the DT Jacobian matrices  \(\tilde{F}\), \(\tilde{G}\) and \(\tilde{H}\) derived in the previous section.  For our linearized filter we suppose that our nonlinear system stays relatively close to a nominal trajectory we will call \(x^*(t)\). The nominal trajectory is obtained by numerically integrating the nonlinear CT dynamics.

Importantly, for the linearized KF, we are estimating the perturbations from this nominal trajectory at each time step, not the full state.  For this model we assume zero process noise input.
At each time step, the filter reconstructs the total state estimate by adding the nominal trajectory, \(\ x^*\) to the estimated perturbation, \(\delta x\) using the following equation:
\[ x_{k+1} \approx x^*_{k+1} + \delta x_{k+1} \]
The linearized KF algorithm uses two major steps, the prediction step and the measurement update step:

Prediction dynamics steps:
\[\delta x_{k+1}^- = \tilde{F}_k \delta x_k^+ + \tilde{G}_k \delta u_k\]
\[P_{k+1}^- = \tilde{F}_k P_k^+ \tilde{F}_k^T + \tilde{\Omega}_k Q_k \tilde{\Omega}_k^T\]

Measurement update steps:
\[\delta x_{k+1}^+ = \delta x_{k+1}^- + K_{k+1}\left(\delta y_{k+1} - \tilde{H}_{k+1}\delta x_{k+1}^- \right)\]
\[K_{k+1} = P_{k+1}^- \tilde{H}_{k+1}^{T} \left(\tilde{H}_{k+1} P_{k+1}^- \tilde{H}_{k+1}^{T} + R_{k+1}\right)^{-1}\]
\[P_{k+1}^+ = \left(I - K_{k+1}\tilde{H}_{k+1}\right) P_{k+1}^-\]

Notice that the prediction step introduces process noise $Q_{\text{k}}$ into the filter.  Similarly the measurment update step introduces the measurement noise covariance $R_{\text{k}}$ into the Kalman gain matrix.

At each time step $k$, we evaluate the CT Jacobians of the dynamics and measurements along the nominal trajectory and construct the DT state transition matrix using the Eulerized approximation.
\[\tilde{F}_k = I + \Delta t\,A_k\]
This method is used instead of the previously used matrix exponential method  for finding $\tilde{F}_k$ in the linearized KF.
Since the nominal control input is equal to the actual applied control, we get $\delta u_k = 0$, so for the prediction step, the perturbation dynamics simplify to \[\delta x_{k+1}^- = \tilde{F}_k \,\delta x_k^+\]

Once the linearized KF is implemented, it is then validated using Monte Carlo Truth Model Tests (TMT) and the NEES and NIS chi-square consistency tests.  The results from these tests were then used to tune the Q and R matrices to obtain the results shown later in this section.

\subsection{Typical Simulation Instance (Part 4a)}
Figure ~\ref{fig:Simulated Data vs True Data} shows a single simulation instance of the states of the UGV and UAV.  We can see in every state that early in the simulation the KF estimate matches well with the truth model test (TMT), however at roughly the 10 second mark, the filter estimate diverges.  This is due to several factors mainly the accumulation of error and process noise.  In several of the plots the filter can be seen alternating between over and under-estimating the ground truth data.  Additionally, an under-tuned process noise covariance matrix $Q_{\text{KF}}$ is likely contributing to this behavior.


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{part2_4a_plot.png}
    \caption{Typical Simulation Instance}
    \label{fig:Simulated Data vs True Data}
\end{figure}

\subsection{NEES Chi-Square Test (Part 4b)}
There are two Chi-square hypothesis tests that we used to check if our KF's state errors and measurement residuals make sense for our models, NEES and NIS.  The Normalized Estimation Error Squared (NEES) test uses our TMT to assess validity of NEES at every time step over the monte carlo simulations.  For each timestep $k$, we run $N$ discrete truth model simulations.  Each simulation includes its own process and measurement noise.

The NEES statistic used for Monte Carlo run $i$ is the following equation for the state estimation error at time $k$:
\[\epsilon^{(i)}_{x,k} = e^{(i)}_{k}{}^{T}\, P_k^{-1}\, e^{(i)}_{k}\]

The sample mean NEES over $N$ runs is:
\[\bar{\epsilon}_{x,k} = \frac{1}{N} \sum_{i=1}^N \epsilon^{(i)}_{x,k}\]

The chi-square consistency bounds are the following:
\[r_1 = \frac{1}{N}\chi^2_{\alpha/2,\, N n_x}, 
\qquad r_2 = \frac{1}{N}\chi^2_{1-\alpha/2,\, N n_x}\]
with significance level $\alpha = 0.05$.  $\bar{\varepsilon}_{x,k}$ should be in the interval $r_1 \le \bar{\varepsilon}_{x,k} \le r_2$ for $(1-\alpha)\times 100\%$ of the time, otherwise the filter is said to be statistically inconsistent.


\begin{figure}[h]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{part2_4b_plot_zoomin.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{part2_4b_plot_zoomout.png}
    \end{minipage}
    \caption{NEES Test Statistic Points vs Time}
    \label{fig:NEES}
\end{figure}

In Figure~\ref{fig:NEES}, the mean NEES quickly grows larger than the upper bound, $r_2$ and remains there for the remainder of trajectory.  This behavior indicates that the linearized KF is over confident and that the actual errors are larger than those predicted by $P_k$.  The process noise covariance matrix $Q_{\text{KF}}$ is too small and is quickly accumulating linearization error as the truth model diverges from the nominal trajectory. Two versions of the same plot are shown with different zoom levels in Figure~\ref{fig:NEES} to show both early and overall behavior of the trajectory.

\subsection{NIS Chi-Square Test (Part 4c)}
The Normalized Innovation Squared (NIS) test uses real (or simulated) sensor data to assess validity of NIS at every time step over the monte carlo simulations.
The innovation statistic is:
\[\epsilon_{y,k}^{(i)} =
\delta y_k^{(i)T} S_k^{-1} \delta y_k^{(i)}\]
with
\[S_k = \tilde{H}_k\, P_k^-\, \tilde{H}_k^{T} + R_k\]

The sample mean is:
\[\bar{\epsilon}_{y,k} = \frac{1}{N} \sum_{i=1}^N \epsilon^{(i)}_{y,k}\]

With measurement dimension $m=5$, the chi-square bounds are:
\[r^{NIS}_1 = \chi^2_{\alpha/2,\, m}\qquad
r^{NIS}_2 = \chi^2_{1-\alpha/2,\, m}\]

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{part2_4c_plot.png}
    \caption{NIS Test Statistic Points vs Time}
    \label{fig:NIS}
\end{figure}

In Figure~\ref{fig:NIS} we see that the NIS statistics remain well above the upper bound for nearly the entire trajectory.  This indicates that the LKF is underestimating the measurement uncertainty.  The filter believes that the measurements are more reliable than they really are , so even small differences between predicted and actual measurements produce large NIS values.  This behavior is consistent with the observations of the NEES results.  Since the linearization is performed in reference to a nominal trajectory, it is inaccurate as soon as the true nonlinear trajectory diverges from that nominal trajectory.

\subsection{Tuning the Linearized Kalman Filter}
Tuning the linearized KF involved applying tuning factors to the Q and R covariance matrices. The process noise covariance $Q_{\text{KF}}$ and measurement noise covariance
$R_{\text{KF}}$ used in the LKF are not assumed to be equal to the true covariances
$Q_{\text{true}}$ and $R_{\text{true}}$.  Instead, we introduce tuning factors
$q_{\text{tune}}$ and $r_{\text{tune}}$ and set
\[
  Q_{\text{KF}} = q_{\text{tune}}\, Q_{\text{true}}, \qquad
  R_{\text{KF}} = r_{\text{tune}}\, R_{\text{true}}.
\]
Starting from $q_{\text{tune}} = r_{\text{tune}} = 1$, we adjusted $q_{\text{tune}}$
using the NEES results: when the NEES remained above the upper bound, we increased $q_{\text{tune}}$ to inflate $Q_{\text{KF}}$ and make the filter less confident in its process model.  A value of $q_{\text{tune}}\ = 10,000,000$ was used to obtain the NEES plot shown in Figure~\ref{fig:NEES}.  Similarly, we adjusted $r_{\text{tune}}$ using the NIS results:
NIS values well above the upper bound indicated that $R_{\text{KF}}$ was too large, so
$r_{\text{tune}}$ was decreased until the NIS approached the consistency interval.  This occurred around $r_{\text{tune}} = 0.001$, but then the NEES plots were well above its bounds, negating previous tuning attempts at that consistency test.


In the final configuration used for the plots shown in this report, we used values of
\[  q_{\text{tune}} = 20, \qquad r_{\text{tune}} = 1\]
These compromised values were found to be the best available compromise after many attempts at tuning both NEES and NIS models.  The plots in Figure~\ref{fig:NEES} and Figure~\ref{fig:NIS} show that even with best-effort tuning of the Q and R matrices, the actual true system trajectory, \(\ x(t)\) deviates quickly from the nominal trajectory \(\ x^*\) causing large errors that become unrecoverable.  This behavior led to the need for an alternative method where we estimate total state and not just the perturbations.

Because the LKF estimates perturbations about a nominal trajectory, any sustained drift between the true nonlinear trajectory and the nominal path causes a mismatch between the linearized model and the real dynamics. This linearization error accumulates in the state estimate, eventually dominating the stochastic noise terms and causing the filter to diverge.

Overall, the LKF performs decently well only when the system remains close to the nominal trajectory and when the linear model provides an adequate local approximation. For this cooperative localization problem, the nonlinear geometric relationships and diverging trajectories result in rapid accumulation of linearization error, ultimately motivating the transition to an Extended Kalman Filter (EKF) that operates on the full nonlinear state, not perturbations.


\section{Stochastic Nonlinear Filtering: Extended Kalman Filter (EKF)}

\subsection{Implementation of the Extended Kalman Filter}

The process model is
\[
x_{k+1} = f(x_k, u_k)\,\Delta t + w_k,
\qquad w_k \sim \mathcal{N}(0, Q_k)\]
and the measurement model is
\[y_k = h(x_k) + v_k,
\qquadv_k \sim \mathcal{N}(0, R_k)\]

Because $f(\cdot)$ and $h(\cdot)$ are nonlinear, the EKF computes Jacobians at each step:
\[F_k = \left. \frac{\partial f}{\partial x} \right|_{x=\hat{x}_k^{+}},
\qquad H_k = \left. \frac{\partial h}{\partial x} \right|_{x=\hat{x}_k^{-}}\]

\subsubsection*{Prediction Equations}

The a priori state and covariance predictions are
\[\hat{x}_{k+1}^{-} = \hat{x}_{k}^{+} + f(\hat{x}_{k}^{+},u_k)\,\Delta t\]
\[P_{k+1}^{-} = F_k\,P_k^{+}\,F_k^{T} + Q_k\]

\subsubsection*{Correction (Measurement Update) Equations}

The innovation and its covariance are
\[\tilde{y}_{k+1} = y_{k+1} - h(\hat{x}_{k+1}^{-})\]
\[S_{k+1} = H_{k+1}\,P_{k+1}^{-}\,H_{k+1}^{T} + R_{k+1}\]

The Kalman gain is computed as
\[K_{k+1} = P_{k+1}^{-}\,H_{k+1}^{T}\,S_{k+1}^{-1}\]
which yields the posterior update equations
\[\hat{x}_{k+1}^{+} = \hat{x}_{k+1}^{-} + K_{k+1}\tilde{y}_{k+1}\]
\[P_{k+1}^{+} = \left(I - K_{k+1} H_{k+1} \right)P_{k+1}^{-}\]

\subsection{Typical Simulation Instance (Part 5a)}

\subsection{NEES Chi-Square Consistency Test (Part 5b)}

The Normalized Estimation Error Squared (NEES) quantifies whether the EKF's internal covariance $P_k$ is statistically consistent with the actual estimation errors.

For Monte Carlo run $i$, the error is
\[e^{(i)}_{k} = x_{k}^{\text{truth},(i)} - \hat{x}_{k}^{(i)}\]
and the NEES statistic is
\[\epsilon^{(i)}_{x,k} = e_{k}^{(i)T} P_k^{-1} e_{k}^{(i)}\]

Averaging over $N$ Monte Carlo runs gives
\[\bar{\epsilon}_{x,k} = \frac{1}{N} \sum_{i=1}^N \epsilon^{(i)}_{x,k}\]

The chi-square lower and upper bounds for confidence level $1-\alpha$ are
\[r_1 = \frac{1}{N}\chi^2_{\alpha/2,\,N n_x}\qquad
r_2 = \frac{1}{N}\chi^2_{1-\alpha/2,\,N n_x}\]
where $n_x = 6$ is the state dimension.

\subsection{NIS Chi-Square Consistency Test (Part 5c)}

The Normalized Innovation Squared (NIS) assesses whether the innovation covariance $S_k$ is consistent with the measurement residuals.

For Monte Carlo trial $i$:
\[\epsilon^{(i)}_{y,k} = \tilde{y}^{(i)T}_k\, S_k^{-1}\, \tilde{y}^{(i)}_k\]
and the sample mean is
\[\bar{\epsilon}_{y,k} = \frac{1}{N}\sum_{i=1}^N \epsilon^{(i)}_{y,k}\]

For measurement dimension $m=5$, the chi-square bounds are
\[r^{NIS}_1 = \frac{1}{N}\chi^2_{\alpha/2,\,Nm}\qquad
r^{NIS}_2 = \frac{1}{N}\chi^2_{1-\alpha/2,\,Nm}\]


\subsection{Tuning the EKF}

The EKF tuning procedure followed the recommendations in the lecture notes...

\section{LKF and EKF Comparison}


\section{Contributions}
Each team member is individually working through the project to allow us to apply what we have learned and attempt to solidify our understanding of the course concepts. The code and images in this report come from Aaron's branch in the git repo.
\url{https://github.com/A-aron2014/ASEN5044-Cooperative-Air-Ground-Robot-Localization}

\end{document}

